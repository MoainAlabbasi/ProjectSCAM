#!/usr/bin/env python3
"""
ุงุฎุชุจุงุฑ ุฎุฏูุฉ Gemini - ูุณุฎุฉ ูุณุชููุฉ
S-ACM - Smart Academic Content Management System

ูุฐุง ุงูุณูุฑูุจุช ูุฎุชุจุฑ ุงูุงุชุตุงู ุจู Gemini API ูุจุงุดุฑุฉ ุจุฏูู Django
"""

from google import genai
from google.genai import types

# ========== Configuration ==========
GEMINI_API_KEY = "AIzaSyC0caScddcPQHxN2fQUSYj02sZ66MG-_80"
GEMINI_MODEL = "gemini-2.5-flash"


def print_header(title: str):
    """ุทุจุงุนุฉ ุนููุงู."""
    print("\n" + "=" * 60)
    print(f"  {title}")
    print("=" * 60)


def print_result(success: bool, message: str):
    """ุทุจุงุนุฉ ุงููุชูุฌุฉ."""
    status = "โ" if success else "โ"
    print(f"{status} {message}")


def main():
    print_header("ุงุฎุชุจุงุฑ Google Gemini API - S-ACM")
    
    # ุชููุฆุฉ ุงูุนููู
    print("\n๐ฆ ุชููุฆุฉ ุงูุนููู...")
    try:
        client = genai.Client(api_key=GEMINI_API_KEY)
        print_result(True, "ุชู ุชููุฆุฉ ุงูุนููู ุจูุฌุงุญ")
    except Exception as e:
        print_result(False, f"ูุดู ุชููุฆุฉ ุงูุนููู: {e}")
        return
    
    # ุงุฎุชุจุงุฑ ุงูุงุชุตุงู
    print_header("ุงุฎุชุจุงุฑ 1: ุงูุงุชุตุงู ุงูุฃุณุงุณู")
    try:
        response = client.models.generate_content(
            model=GEMINI_MODEL,
            contents="ูู: ูุฑุญุจุงูุ ุฃูุง ุฌุงูุฒ ููุนูู!",
            config=types.GenerateContentConfig(
                max_output_tokens=100,
                temperature=0.3,
            )
        )
        print_result(True, "ุงูุงุชุตุงู ูุงุฌุญ!")
        print(f"   ุงูุฑุฏ: {response.text}")
    except Exception as e:
        print_result(False, f"ูุดู ุงูุงุชุตุงู: {e}")
        return
    
    # ูุต ููุงุฎุชุจุงุฑ
    test_text = """
    ุงูุฐูุงุก ุงูุงุตุทูุงุนู (AI) ูู ูุฑุน ูู ุนููู ุงูุญุงุณูุจ ููุฏู ุฅูู ุฅูุดุงุก ุฃูุธูุฉ ูุงุฏุฑุฉ ุนูู ุฃุฏุงุก ููุงู 
    ุชุชุทูุจ ุนุงุฏุฉู ุฐูุงุกู ุจุดุฑูุงู. ูุดูู ุฐูู ุงูุชุนูู ุงูุขููุ ููุนุงูุฌุฉ ุงููุบุฉ ุงูุทุจูุนูุฉุ ูุงูุฑุคูุฉ ุงูุญุงุณูุจูุฉ.
    
    ุงูุชุนูู ุงูุขูู ูู ุฃุญุฏ ุฃูู ูุฑูุน ุงูุฐูุงุก ุงูุงุตุทูุงุนูุ ุญูุซ ุชุชุนูู ุงูุฃูุธูุฉ ูู ุงูุจูุงูุงุช ุจุฏูุงู ูู 
    ุงูุจุฑูุฌุฉ ุงูุตุฑูุญุฉ. ููุณุชุฎุฏู ูู ุชุทุจููุงุช ูุชุนุฏุฏุฉ ูุซู ุงูุชุนุฑู ุนูู ุงูุตูุฑ ูุงูุชูุจุค ุจุงูุฃุณุนุงุฑ.
    """
    
    # ุงุฎุชุจุงุฑ ุงูุชูุฎูุต
    print_header("ุงุฎุชุจุงุฑ 2: ุชูููุฏ ุงูุชูุฎูุต")
    try:
        prompt = f"""ุฃูุช ูุณุงุนุฏ ุฃูุงุฏููู. ูู ุจุชูุฎูุต ุงููุต ุงูุชุงูู ุจุงููุบุฉ ุงูุนุฑุจูุฉ ูู 3 ุฌูู:

{test_text}

ุงูุชูุฎูุต:"""
        
        response = client.models.generate_content(
            model=GEMINI_MODEL,
            contents=prompt,
            config=types.GenerateContentConfig(
                max_output_tokens=300,
                temperature=0.3,
            )
        )
        print_result(True, "ุชู ุชูููุฏ ุงูุชูุฎูุต ุจูุฌุงุญ!")
        print(f"\n๐ ุงูุชูุฎูุต:\n{response.text}")
    except Exception as e:
        print_result(False, f"ูุดู ุชูููุฏ ุงูุชูุฎูุต: {e}")
    
    # ุงุฎุชุจุงุฑ ุชูููุฏ ุงูุฃุณุฆูุฉ
    print_header("ุงุฎุชุจุงุฑ 3: ุชูููุฏ ุงูุฃุณุฆูุฉ")
    try:
        prompt = f"""ุฃูุช ูุฏุฑุณ. ุฃูุดุฆ ุณุคุงููู ุงุฎุชูุงุฑ ูู ูุชุนุฏุฏ ูู ุงููุต ุงูุชุงูู.
ุฃุฑุฌุน ุงูุฅุฌุงุจุฉ ุจุตูุบุฉ JSON ููุท:

ุงููุต:
{test_text}

ุงูุฃุณุฆูุฉ (JSON):"""
        
        response = client.models.generate_content(
            model=GEMINI_MODEL,
            contents=prompt,
            config=types.GenerateContentConfig(
                max_output_tokens=500,
                temperature=0.5,
            )
        )
        print_result(True, "ุชู ุชูููุฏ ุงูุฃุณุฆูุฉ ุจูุฌุงุญ!")
        print(f"\nโ ุงูุฃุณุฆูุฉ:\n{response.text}")
    except Exception as e:
        print_result(False, f"ูุดู ุชูููุฏ ุงูุฃุณุฆูุฉ: {e}")
    
    # ุงุฎุชุจุงุฑ ุณุคุงู ุงููุณุชูุฏ
    print_header("ุงุฎุชุจุงุฑ 4: ุณุคุงู ุงููุณุชูุฏ")
    try:
        question = "ูุง ูู ุฃูู ูุฑูุน ุงูุฐูุงุก ุงูุงุตุทูุงุนู ุงููุฐููุฑุฉุ"
        prompt = f"""ุฃุฌุจ ุนูู ุงูุณุคุงู ุงูุชุงูู ุจูุงุกู ุนูู ุงููุต:

ุงููุต:
{test_text}

ุงูุณุคุงู: {question}

ุงูุฅุฌุงุจุฉ:"""
        
        response = client.models.generate_content(
            model=GEMINI_MODEL,
            contents=prompt,
            config=types.GenerateContentConfig(
                max_output_tokens=200,
                temperature=0.3,
            )
        )
        print_result(True, "ุชู ุงูุฅุฌุงุจุฉ ุนูู ุงูุณุคุงู ุจูุฌุงุญ!")
        print(f"\nโ ุงูุณุคุงู: {question}")
        print(f"๐ฌ ุงูุฅุฌุงุจุฉ: {response.text}")
    except Exception as e:
        print_result(False, f"ูุดู ุงูุฅุฌุงุจุฉ ุนูู ุงูุณุคุงู: {e}")
    
    print_header("ุงูุชูุงุก ุงูุงุฎุชุจุงุฑุงุช")
    print("โ ุฌููุน ุงูุงุฎุชุจุงุฑุงุช ุงูุชููุช ุจูุฌุงุญ!")
    print(f"\n๐ ุงูููุฏูู ุงููุณุชุฎุฏู: {GEMINI_MODEL}")


if __name__ == "__main__":
    main()
